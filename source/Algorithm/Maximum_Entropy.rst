最大熵模型 ME（Maximum Entropy）
================================

我的这篇文章介绍了信息熵的概念，信息熵越大不确定性也就越大，信息熵最大时表示各种概率的均等分布，也就是个不偏不倚的猜测，最大熵模型一般就是在已知条件下，来求是的熵最大的情况，最大熵模型我们一般会有
feature 函数，给定的条件就是样本期望等于模型期望，即

p‾‾(f)=Σp‾‾(ai,ci)∗f(ai,ci)=p(f)=Σp(ci∣∣ai)∗p‾‾(ai)∗f(ai,ci)

在已知条件下就是求熵最大的情况

argmaxH(ci∣∣ai)

H 就是信息熵的函数，于是这样我们就求出了 P(ci∣∣ai)，就知道了每个字 a
的标注 c 了，最大熵模型的一个好处是我们可以引入各种各样的
feature，而不仅仅是从字出现的频率去分词，比如我们可以加入 domain
knowledge，可以加入已知的字典信息等。
