

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>28. æ¨¡å‹ &mdash; nlp-docs v2019.03.19 æ–‡æ¡£</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
    <link rel="index" title="ç´¢å¼•" href="../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../search.html" />
    <link rel="next" title="29. å†œä¸šçŸ¥è¯†å›¾è°±(AgriKG)" href="../Knowledge_Graph/Agricultural_Knowledge_Graph.html" />
    <link rel="prev" title="27. BERT å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../contents.html" class="icon icon-home"> nlp-docs
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../info.html">1. è‡ªç„¶è¯­è¨€å¤„ç†</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Author/index.html">2. è‡ªç„¶è¯­è¨€å¤„ç†ä½œè€…</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/ConditionalRandomField.html">3. æ¡ä»¶éšæœºåœº CRFï¼ˆConditional Random Fieldï¼‰</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Hidden_Markov_Model.html">4. éšé©¬å°”å¯å¤«æ¨¡å‹ HMMï¼ˆHidden Markov Modelï¼‰</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/MMSEG.html">5. MMSEG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/MMSeg_description.html">6. MMSeg åˆ†è¯ç®—æ³•ç®€è¿°</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Maximum-entropy_Markov_model.html">7. æœ€å¤§ç†µé©¬å°”å¯å¤«æ¨¡å‹ MEMMï¼ˆMaximum-entropy Markov modelï¼‰</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Maximum_Entropy.html">8. æœ€å¤§ç†µæ¨¡å‹ MEï¼ˆMaximum Entropyï¼‰</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Statistical_Model.html">9. ç»Ÿè®¡æ¨¡å‹</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Viterbi.html">10. Viterbi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/index.html">11. ç®—æ³•æ±‡æ€»</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/Awesome-Chinese-NLP.html">12. awesome-chinese-nlp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/index.html">13. awesome-nlp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/multi-language.html">14. å…¶ä»–è¯­è¨€</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/techniques.html">15. æŠ€æœ¯</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/tutorials.html">16. æ•™ç¨‹</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Cloud/aliyun.html">17. é˜¿é‡Œäº‘è‡ªç„¶è¯­è¨€å¤„ç†</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Cloud/pai.html">18. é˜¿é‡Œæœºå™¨å­¦ä¹ å¹³å° PAI 3.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Cloud/ç™¾åº¦AI.html">19. ç™¾åº¦ AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">20. å¸¸é—®é—®é¢˜</a></li>
<li class="toctree-l1"><a class="reference internal" href="Fine-tuning_with_BERT.html">21. ä½¿ç”¨ BERT è¿›è¡Œå¾®è°ƒ</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pre-trained_models.html">22. é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pre-training_with_BERT.html">23. ä½¿ç”¨ BERT è¿›è¡Œé¢„è®­ç»ƒ</a></li>
<li class="toctree-l1"><a class="reference internal" href="Using_BERT_in_Colab.html">24. åœ¨ Colab ä¸­ä½¿ç”¨ BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="Using_BERT_to_extract_fixed_feature_vectors.html">25. ä½¿ç”¨ BERT æå–å›ºå®šçš„ç‰¹å¾å‘é‡ (åƒ ELMo)</a></li>
<li class="toctree-l1"><a class="reference internal" href="What_is_BERT.html">26. ä»€ä¹ˆæ˜¯ BERTï¼Ÿ</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">27. BERT å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">28. æ¨¡å‹</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">28.1. ç»“æœ</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">28.2. å¾®è°ƒç¤ºä¾‹</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id4">28.3. ç»†èŠ‚</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id5">28.3.1. æ•°æ®æºå’Œé‡‡æ ·</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">28.3.2. ç¬¦å·åŒ–</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">28.3.3. è¯­è¨€æ¸…å•</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/Agricultural_Knowledge_Graph.html">29. å†œä¸šçŸ¥è¯†å›¾è°±(AgriKG)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/ChineseNLPCorpus.html">30. Chinese NLP Corpus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/cnSchema.html">31. cnSchema</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/index.html">32. çŸ¥è¯†å›¾è°±</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/openkg.html">33. openkg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regulation/LabelStyle.html">34. ç°ä»£æ±‰è¯­è¯­æ–™åº“åŠ å·¥è§„èŒƒ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regulation/index.html">35. æ ‡æ³¨è§„èŒƒ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regulation/nation.html">36. ä¿¡æ¯å¤„ç†ç”¨ç°ä»£æ±‰è¯­è¯ç±»æ ‡è®°è§„èŒƒ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regulation/nlpir.html">37. è®¡ç®—æ‰€æ±‰è¯­è¯æ€§æ ‡è®°é›†</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GlobalWordNetAssociation.html">38. å…¨çƒ WordNet åä¼š</a></li>
<li class="toctree-l1"><a class="reference internal" href="../HowNet.html">39. çŸ¥ç½‘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Schema.html">40. Schema</a></li>
<li class="toctree-l1"><a class="reference internal" href="../aihanyu.html">41. çˆ±æ±‰è¯­è¯­æ–™åº“</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cow.html">42. Chinese Open Wordnet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csdn.html">43. CSDN ä¸‹è½½</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dianchacha.html">44. åº—æŸ¥æŸ¥æ•°æ®</a></li>
<li class="toctree-l1"><a class="reference internal" href="../funNLP.html">45. funNLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html">46. è¯­æ–™åº“</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nlp-datasets.html">47. nlp-datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pku-opendata.html">48. åŒ—äº¬å¤§å­¦å¼€å‘æ•°æ®ç ”ç©¶å¹³å°</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wikipedia.html">49. ç»´åŸºç™¾ç§‘è¯­æ–™åº“</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wordnet.html">50. WordNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/FoolNLTK/index.html">51. FoolNLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/FoolNLTK/train.html">52. train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/JieBa/Jieba.html">53. JieBa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/JieBa/JiebaCpp.html">54. CppJieba</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/JieBa/JiebaNode.html">55. NodeJieba</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/HanLP.html">56. HanLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/Jcseg.html">57. Jcseg logo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/Jiagu.html">58. Jiagu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/Keras.html">59. Keras</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/Keras.html#x-train-and-y-train-are-numpy-arrays-just-like-in-the-scikit-learn-api">60. x_train and y_train are Numpy arrays â€“just like in the Scikit-Learn API.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/PyTorch.html">61. PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/SnowNLP.html">62. SnowNLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/SpaCy.html">63. spacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/StanfordNLP.html">64. æ–¯å¦ç¦-StanfordNLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/THULAC.html">65. æ¸…å-THULAC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/TensorFlow.html">66. TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/ansj_seg.html">67. Ansj ä¸­æ–‡åˆ†è¯</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/awesome.html">68. ğŸ‘ğŸ» è¡¨åº“</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/bosonnlp.html">69. ç»æ£®æ•°æ®</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/index.html">70. å¸¸ç”¨åˆ†è¯å·¥å…·åŒ…</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/ltp.html">71. å“ˆå·¥å¤§-LTP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/nlpair-ictclas.html">72. ä¸­ç§‘é™¢-NLPIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/nltk.html">73. NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/pkuseg.html">74. åŒ—å¤§-pkuseg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/scikit-learn.html">75. scikit-learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/sego.html">76. sego</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Online/index.html">77. åœ¨çº¿åˆ†æå·¥å…·</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/2018-nlp.html">78. 2018 å¹´ï¼ŒNLP ç ”ç©¶ä¸åº”ç”¨è¿›å±•åˆ°ä»€ä¹ˆæ°´å¹³äº†ï¼Ÿ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/bert-jiqizhixin.html">79. è°·æ­Œç»ˆäºå¼€æº BERT ä»£ç ï¼š3 äº¿å‚æ•°é‡ï¼Œæœºå™¨ä¹‹å¿ƒå…¨é¢è§£è¯»</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/chinese-segmenter.html">80. ç»†è¯´ä¸­æ–‡åˆ†è¯</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/index.html">81. è®ºæ–‡ || æ–‡ç« </a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/milestone.html">82. ä¸€æ–‡çœ‹æ‡‚ NLP ç¥ç»ç½‘ç»œå‘å±•å†å²ä¸­æœ€é‡è¦çš„ 8 ä¸ªé‡Œç¨‹ç¢‘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/nlp-gather.html">83. è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çŸ¥è¯†ç»“æ„æ€»ç»“</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/nlp.ict.ac.cn.html">84. ä¸­å›½ç§‘å­¦é™¢è®¡ç®—æŠ€æœ¯ç ”ç©¶æ‰€è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶ç»„</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sensitive-word/fastscan.html">85. FastScan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sensitive-word/index.html">86. æ•æ„Ÿè¯</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">87. æœ¯è¯­è¡¨</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../contents.html">nlp-docs</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../contents.html">Docs</a> &raquo;</li>
        
      <li>28. æ¨¡å‹</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/Corpus/BERT/model.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1>28. æ¨¡å‹<a class="headerlink" href="#id1" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">Â¶</a></h1>
<p>ç›®å‰æœ‰ä¸¤ç§å¤šè¯­è¨€æ¨¡å‹å¯ä¾›é€‰æ‹©ã€‚
æˆ‘ä»¬ä¸æ‰“ç®—å‘å¸ƒæ›´å¤šçš„å•è¯­è¨€æ¨¡å‹ï¼Œä½†æˆ‘ä»¬å°†æ¥å¯èƒ½ä¼šå‘å¸ƒè¿™ä¸¤ç§ç‰ˆæœ¬çš„â€œBERT-Largeâ€ç‰ˆæœ¬:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">`BERT-Base,</span> <span class="pre">å¤šè¯­å¥—è£…(æ–°æ¨è)</span></code> &lt;<a class="reference external" href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip">https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip</a>&gt;`__:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">`BERT-Base,</span> <span class="pre">å¤šè¯­è¨€Uncased(Origï¼Œä¸æ¨è)</span></code> &lt;<a class="reference external" href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip">https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip</a>&gt;`__:
102 languages, 12-layer, 768-hidden, 12-heads, 110M parameters</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">`BERT-Base,</span> <span class="pre">ä¸­æ–‡</span></code> &lt;<a class="reference external" href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip">https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip</a>&gt;`__:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads,
110M parameters</p></li>
</ul>
<p><strong>â€œå¤šè¯­è¨€å¥—è£…(æ–°)â€æ¨¡å‹è¿˜ä¿®å¤äº†è®¸å¤šè¯­è¨€ä¸­çš„è§„èŒƒåŒ–é—®é¢˜ï¼Œå› æ­¤å»ºè®®ä½¿ç”¨éæ‹‰ä¸å­—æ¯è¡¨çš„è¯­è¨€(å¯¹äºå¤§å¤šæ•°ä½¿ç”¨æ‹‰ä¸å­—æ¯çš„è¯­è¨€ï¼Œå®ƒé€šå¸¸æ›´å¥½)ã€‚
ä½¿ç”¨æ­¤æ¨¡å‹æ—¶ï¼Œè¯·ç¡®ä¿å°†``â€“do_lower_case = false``ä¼ é€’ç»™``run_pretraining.py``å’Œå…¶ä»–è„šæœ¬ã€‚</strong></p>
<p>è¯·å‚é˜…å¤šè¯­è¨€æ¨¡å‹æ”¯æŒçš„<a class="reference external" href="ï¼ƒlist-of-languages">è¯­è¨€åˆ—è¡¨</a>ã€‚
å¤šè¯­è¨€æ¨¡å‹ç¡®å®åŒ…å«ä¸­æ–‡(å’Œè‹±æ–‡)ï¼Œä½†å¦‚æœæ‚¨çš„å¾®è°ƒæ•°æ®ä»…é™ä¸­æ–‡ï¼Œåˆ™ä¸­æ–‡æ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚</p>
<div class="section" id="id2">
<h2>28.1. ç»“æœ<a class="headerlink" href="#id2" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">Â¶</a></h2>
<p>ä¸ºäº†è¯„ä¼°è¿™äº›ç³»ç»Ÿï¼Œæˆ‘ä»¬ä½¿ç”¨<a class="reference external" href="https://github.com/facebookresearch/XNLI">XNLI
æ•°æ®é›†</a>æ•°æ®é›†ï¼Œè¿™æ˜¯<a class="reference external" href="https://www.nyu.edu/projects/bowman/multinli/">MultiNLI</a>çš„ä¸€ä¸ªç‰ˆæœ¬å…¶ä¸­å¼€å‘å’Œæµ‹è¯•é›†å·²ç»(ç”±äººç±»)ç¿»è¯‘æˆ
15 ç§è¯­è¨€ã€‚ è¯·æ³¨æ„ï¼Œè®­ç»ƒé›†æ˜¯<em>æœºå™¨</em>ç¿»è¯‘(æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ XNLI
æä¾›çš„ç¿»è¯‘ï¼Œè€Œä¸æ˜¯ Google NMT)ã€‚ ä¸ºæ¸…æ¥šèµ·è§ï¼Œæˆ‘ä»¬ä»…æŠ¥å‘Šä»¥ä¸‹ 6 ç§è¯­è¨€:</p>
<!-- mdformat off(æ²¡æœ‰è¡¨æ ¼) --><!-- mdformat on --><p>The first two rows are baselines from the XNLI paper and the last three
rows are our results with BERT.</p>
<p><strong>Translate Train</strong> means that the MultiNLI training set was machine
translated from English into the foreign language. So training and
evaluation were both done in the foreign language. Unfortunately,
training was done on machine-translated data, so it is impossible to
quantify how much of the lower accuracy (compared to English) is due to
the quality of the machine translation vs.Â the quality of the
pre-trained model.</p>
<p><strong>Translate Test</strong> means that the XNLI test set was machine translated
from the foreign language into English. So training and evaluation were
both done on English. However, test evaluation was done on
machine-translated English, so the accuracy depends on the quality of
the machine translation system.</p>
<p><strong>Zero Shot</strong> means that the Multilingual BERT system was fine-tuned on
English MultiNLI, and then evaluated on the foreign language XNLI test.
In this case, machine translation was not involved at all in either the
pre-training or fine-tuning.</p>
<p>Note that the English result is worse than the 84.2 MultiNLI baseline
because this training used Multilingual BERT rather than English-only
BERT. This implies that for high-resource languages, the Multilingual
model is somewhat worse than a single-language model. However, it is not
feasible for us to train and maintain dozens of single-language model.
Therefore, if your goal is to maximize performance with a language other
than English or Chinese, you might find it beneficial to run
pre-training for additional steps starting from our Multilingual model
on data from your language of interest.</p>
<p>Here is a comparison of training Chinese models with the Multilingual
<code class="docutils literal notranslate"><span class="pre">BERT-Base</span></code> and Chinese-only <code class="docutils literal notranslate"><span class="pre">BERT-Base</span></code>:</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 74%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>System</p></th>
<th class="head"><p>Chinese</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>XNLI Baseline</p></td>
<td><p>67.0</p></td>
</tr>
<tr class="row-odd"><td><p>BERT Multilingual Model</p></td>
<td><p>74.2</p></td>
</tr>
<tr class="row-even"><td><p>BERT Chinese-only Model</p></td>
<td><p>77.2</p></td>
</tr>
</tbody>
</table>
<p>Similar to English, the single-language model does 3% better than the
Multilingual model.</p>
</div>
<div class="section" id="id3">
<h2>28.2. å¾®è°ƒç¤ºä¾‹<a class="headerlink" href="#id3" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">Â¶</a></h2>
<p>The multilingual model does <strong>not</strong> require any special consideration or
API changes. We did update the implementation of <code class="docutils literal notranslate"><span class="pre">BasicTokenizer</span></code> in
<code class="docutils literal notranslate"><span class="pre">tokenization.py</span></code> to support Chinese character tokenization, so please
update if you forked it. However, we did not change the tokenization
API.</p>
<p>To test the new models, we did modify <code class="docutils literal notranslate"><span class="pre">run_classifier.py</span></code> to add
support for the <a class="reference external" href="https://github.com/facebookresearch/XNLI">XNLI
dataset</a>. This is a
15-language version of MultiNLI where the dev/test sets have been
human-translated, and the training set has been machine-translated.</p>
<p>To run the fine-tuning code, please download the <a class="reference external" href="https://s3.amazonaws.com/xnli/XNLI-1.0.zip">XNLI dev/test
set</a> and the <a class="reference external" href="https://s3.amazonaws.com/xnli/XNLI-MT-1.0.zip">XNLI
machine-translated training
set</a> and then unpack
both .zip files into some directory <code class="docutils literal notranslate"><span class="pre">$XNLI_DIR</span></code>.</p>
<p>To run fine-tuning on XNLI. The language is hard-coded into
<code class="docutils literal notranslate"><span class="pre">run_classifier.py</span></code> (Chinese by default), so please modify
<code class="docutils literal notranslate"><span class="pre">XnliProcessor</span></code> if you want to run on another language.</p>
<p>This is a large dataset, so this will training will take a few hours on
a GPU (or about 30 minutes on a Cloud TPU). To run an experiment quickly
for debugging, just set <code class="docutils literal notranslate"><span class="pre">num_train_epochs</span></code> to a small value like
<code class="docutils literal notranslate"><span class="pre">0.1</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">BERT_BASE_DIR</span><span class="o">=</span>/path/to/bert/chinese_L-12_H-768_A-12 <span class="c1"># or multilingual_L-12_H-768_A-12</span>
<span class="nb">export</span> <span class="nv">XNLI_DIR</span><span class="o">=</span>/path/to/xnli

python run_classifier.py <span class="se">\</span>
  --task_name<span class="o">=</span>XNLI <span class="se">\</span>
  --do_train<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  --do_eval<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  --data_dir<span class="o">=</span><span class="nv">$XNLI_DIR</span> <span class="se">\</span>
  --vocab_file<span class="o">=</span><span class="nv">$BERT_BASE_DIR</span>/vocab.txt <span class="se">\</span>
  --bert_config_file<span class="o">=</span><span class="nv">$BERT_BASE_DIR</span>/bert_config.json <span class="se">\</span>
  --init_checkpoint<span class="o">=</span><span class="nv">$BERT_BASE_DIR</span>/bert_model.ckpt <span class="se">\</span>
  --max_seq_length<span class="o">=</span><span class="m">128</span> <span class="se">\</span>
  --train_batch_size<span class="o">=</span><span class="m">32</span> <span class="se">\</span>
  --learning_rate<span class="o">=</span>5e-5 <span class="se">\</span>
  --num_train_epochs<span class="o">=</span><span class="m">2</span>.0 <span class="se">\</span>
  --output_dir<span class="o">=</span>/tmp/xnli_output/
</pre></div>
</div>
<p>With the Chinese-only model, the results should look something like
this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="o">*****</span> <span class="n">Eval</span> <span class="n">results</span> <span class="o">*****</span>
<span class="n">eval_accuracy</span> <span class="o">=</span> <span class="mf">0.774116</span>
<span class="n">eval_loss</span> <span class="o">=</span> <span class="mf">0.83554</span>
<span class="n">global_step</span> <span class="o">=</span> <span class="mi">24543</span>
<span class="n">loss</span> <span class="o">=</span> <span class="mf">0.74603</span>
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h2>28.3. ç»†èŠ‚<a class="headerlink" href="#id4" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">Â¶</a></h2>
<div class="section" id="id5">
<h3>28.3.1. æ•°æ®æºå’Œé‡‡æ ·<a class="headerlink" href="#id5" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">Â¶</a></h3>
<p>The languages chosen were the <a class="reference external" href="https://meta.wikimedia.org/wiki/List_of_Wikipedias">top 100 languages with the largest
Wikipedias</a>. The
entire Wikipedia dump for each language (excluding user and talk pages)
was taken as the training data for each language</p>
<p>However, the size of the Wikipedia for a given language varies greatly,
and therefore low-resource languages may be â€œunder-representedâ€ in terms
of the neural network model (under the assumption that languages are
â€œcompetingâ€ for limited model capacity to some extent).</p>
<p>However, the size of a Wikipedia also correlates with the number of
speakers of a language, and we also donâ€™t want to overfit the model by
performing thousands of epochs over a tiny Wikipedia for a particular
language.</p>
<p>To balance these two factors, we performed exponentially smoothed
weighting of the data during pre-training data creation (and WordPiece
vocab creation). In other words, letâ€™s say that the probability of a
language is <em>P(L)</em>, e.g., <em>P(English) = 0.21</em> means that after
concatenating all of the Wikipedias together, 21% of our data is
English. We exponentiate each probability by some factor <em>S</em> and then
re-normalize, and sample from that distribution. In our case we use
<em>S=0.7</em>. So, high-resource languages like English will be under-sampled,
and low-resource languages like Icelandic will be over-sampled. E.g., in
the original distribution English would be sampled 1000x more than
Icelandic, but after smoothing itâ€™s only sampled 100x more.</p>
</div>
<div class="section" id="id6">
<h3>28.3.2. ç¬¦å·åŒ–<a class="headerlink" href="#id6" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">Â¶</a></h3>
<p>For tokenization, we use a 110k shared WordPiece vocabulary. The word
counts are weighted the same way as the data, so low-resource languages
are upweighted by some factor. We intentionally do <em>not</em> use any marker
to denote the input language (so that zero-shot training can work).</p>
<p>Because Chinese (and Japanese Kanji and Korean Hanja) does not have
whitespace characters, we add spaces around every character in the <a class="reference external" href="https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)">CJK
Unicode
range</a>
before applying WordPiece. This means that Chinese is effectively
character-tokenized. Note that the CJK Unicode block only includes
Chinese-origin characters and does <em>not</em> include Hangul Korean or
Katakana/Hiragana Japanese, which are tokenized with
whitespace+WordPiece like all other languages.</p>
<p>For all other languages, we apply the <a class="reference external" href="https://github.com/google-research/bert#tokenization">same recipe as
English</a>: (a)
lower casing+accent removal, (b) punctuation splitting, (c) whitespace
tokenization. We understand that accent markers have substantial meaning
in some languages, but felt that the benefits of reducing the effective
vocabulary make up for this. Generally the strong contextual models of
BERT should make up for any ambiguity introduced by stripping accent
markers.</p>
</div>
<div class="section" id="id7">
<h3>28.3.3. è¯­è¨€æ¸…å•<a class="headerlink" href="#id7" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">Â¶</a></h3>
<p>å¤šè¯­è¨€æ¨¡å‹æ”¯æŒä»¥ä¸‹è¯­è¨€ã€‚ é€‰æ‹©è¿™äº›è¯­è¨€æ˜¯å› ä¸ºå®ƒä»¬æ˜¯å…·æœ‰æœ€å¤§ç»´åŸºç™¾ç§‘çš„å‰
100 ç§è¯­è¨€:</p>
<ul class="simple">
<li><p>Afrikaans</p></li>
<li><p>Albanian</p></li>
<li><p>Arabic</p></li>
<li><p>Aragonese</p></li>
<li><p>Armenian</p></li>
<li><p>Asturian</p></li>
<li><p>Azerbaijani</p></li>
<li><p>Bashkir</p></li>
<li><p>Basque</p></li>
<li><p>Bavarian</p></li>
<li><p>Belarusian</p></li>
<li><p>Bengali</p></li>
<li><p>Bishnupriya Manipuri</p></li>
<li><p>Bosnian</p></li>
<li><p>Breton</p></li>
<li><p>Bulgarian</p></li>
<li><p>Burmese</p></li>
<li><p>Catalan</p></li>
<li><p>Cebuano</p></li>
<li><p>Chechen</p></li>
<li><p>Chinese (Simplified)</p></li>
<li><p>Chinese (Traditional)</p></li>
<li><p>Chuvash</p></li>
<li><p>Croatian</p></li>
<li><p>Czech</p></li>
<li><p>Danish</p></li>
<li><p>Dutch</p></li>
<li><p>English</p></li>
<li><p>Estonian</p></li>
<li><p>Finnish</p></li>
<li><p>French</p></li>
<li><p>Galician</p></li>
<li><p>Georgian</p></li>
<li><p>German</p></li>
<li><p>Greek</p></li>
<li><p>Gujarati</p></li>
<li><p>Haitian</p></li>
<li><p>Hebrew</p></li>
<li><p>Hindi</p></li>
<li><p>Hungarian</p></li>
<li><p>Icelandic</p></li>
<li><p>Ido</p></li>
<li><p>Indonesian</p></li>
<li><p>Irish</p></li>
<li><p>Italian</p></li>
<li><p>Japanese</p></li>
<li><p>Javanese</p></li>
<li><p>Kannada</p></li>
<li><p>Kazakh</p></li>
<li><p>Kirghiz</p></li>
<li><p>Korean</p></li>
<li><p>Latin</p></li>
<li><p>Latvian</p></li>
<li><p>Lithuanian</p></li>
<li><p>Lombard</p></li>
<li><p>Low Saxon</p></li>
<li><p>Luxembourgish</p></li>
<li><p>Macedonian</p></li>
<li><p>Malagasy</p></li>
<li><p>Malay</p></li>
<li><p>Malayalam</p></li>
<li><p>Marathi</p></li>
<li><p>Minangkabau</p></li>
<li><p>Nepali</p></li>
<li><p>Newar</p></li>
<li><p>Norwegian (Bokmal)</p></li>
<li><p>Norwegian (Nynorsk)</p></li>
<li><p>Occitan</p></li>
<li><p>Persian (Farsi)</p></li>
<li><p>Piedmontese</p></li>
<li><p>Polish</p></li>
<li><p>Portuguese</p></li>
<li><p>Punjabi</p></li>
<li><p>Romanian</p></li>
<li><p>Russian</p></li>
<li><p>Scots</p></li>
<li><p>Serbian</p></li>
<li><p>Serbo-Croatian</p></li>
<li><p>Sicilian</p></li>
<li><p>Slovak</p></li>
<li><p>Slovenian</p></li>
<li><p>South Azerbaijani</p></li>
<li><p>Spanish</p></li>
<li><p>Sundanese</p></li>
<li><p>Swahili</p></li>
<li><p>Swedish</p></li>
<li><p>Tagalog</p></li>
<li><p>Tajik</p></li>
<li><p>Tamil</p></li>
<li><p>Tatar</p></li>
<li><p>Telugu</p></li>
<li><p>Turkish</p></li>
<li><p>Ukrainian</p></li>
<li><p>Urdu</p></li>
<li><p>Uzbek</p></li>
<li><p>Vietnamese</p></li>
<li><p>VolapÃ¼k</p></li>
<li><p>Waray-Waray</p></li>
<li><p>Welsh</p></li>
<li><p>West Frisian</p></li>
<li><p>Western Punjabi</p></li>
<li><p>Yoruba</p></li>
</ul>
<p><strong>å¤šè¯­è¨€å¥—è£…(æ–°)</strong>ç‰ˆæœ¬è¿˜åŒ…å«<strong>æ³°å›½</strong>å’Œ<strong>è’™å¤è¯­</strong>ï¼Œè¿™äº›éƒ½æœªåŒ…å«åœ¨åŸå§‹ç‰ˆæœ¬ä¸­ã€‚</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../Knowledge_Graph/Agricultural_Knowledge_Graph.html" class="btn btn-neutral float-right" title="29. å†œä¸šçŸ¥è¯†å›¾è°±(AgriKG)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="27. BERT å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Nosy

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>