

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>28. 模型 &mdash; nlp-docs v2019.03.19 文档</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="29. 农业知识图谱(AgriKG)" href="../Knowledge_Graph/Agricultural_Knowledge_Graph.html" />
    <link rel="prev" title="27. BERT 大规模预训练语言模型" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../contents.html" class="icon icon-home"> nlp-docs
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../info.html">1. 自然语言处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Author/index.html">2. 自然语言处理作者</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/ConditionalRandomField.html">3. 条件随机场 CRF（Conditional Random Field）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Hidden_Markov_Model.html">4. 隐马尔可夫模型 HMM（Hidden Markov Model）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/MMSEG.html">5. MMSEG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/MMSeg_description.html">6. MMSeg 分词算法简述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Maximum-entropy_Markov_model.html">7. 最大熵马尔可夫模型 MEMM（Maximum-entropy Markov model）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Maximum_Entropy.html">8. 最大熵模型 ME（Maximum Entropy）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Statistical_Model.html">9. 统计模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Viterbi.html">10. Viterbi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/index.html">11. 算法汇总</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/Awesome-Chinese-NLP.html">12. awesome-chinese-nlp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/index.html">13. awesome-nlp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/multi-language.html">14. 其他语言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/techniques.html">15. 技术</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/tutorials.html">16. 教程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Cloud/aliyun.html">17. 阿里云自然语言处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Cloud/pai.html">18. 阿里机器学习平台 PAI 3.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Cloud/百度AI.html">19. 百度 AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">20. 常问问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="Fine-tuning_with_BERT.html">21. 使用 BERT 进行微调</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pre-trained_models.html">22. 预先训练的模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pre-training_with_BERT.html">23. 使用 BERT 进行预训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="Using_BERT_in_Colab.html">24. 在 Colab 中使用 BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="Using_BERT_to_extract_fixed_feature_vectors.html">25. 使用 BERT 提取固定的特征向量 (像 ELMo)</a></li>
<li class="toctree-l1"><a class="reference internal" href="What_is_BERT.html">26. 什么是 BERT？</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">27. BERT 大规模预训练语言模型</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">28. 模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">28.1. 结果</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">28.2. 微调示例</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id4">28.3. 细节</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id5">28.3.1. 数据源和采样</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">28.3.2. 符号化</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">28.3.3. 语言清单</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/Agricultural_Knowledge_Graph.html">29. 农业知识图谱(AgriKG)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/ChineseNLPCorpus.html">30. Chinese NLP Corpus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/cnSchema.html">31. cnSchema</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/index.html">32. 知识图谱</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/openkg.html">33. openkg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regulation/LabelStyle.html">34. 现代汉语语料库加工规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regulation/index.html">35. 标注规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regulation/nation.html">36. 信息处理用现代汉语词类标记规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regulation/nlpir.html">37. 计算所汉语词性标记集</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GlobalWordNetAssociation.html">38. 全球 WordNet 协会</a></li>
<li class="toctree-l1"><a class="reference internal" href="../HowNet.html">39. 知网</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Schema.html">40. Schema</a></li>
<li class="toctree-l1"><a class="reference internal" href="../aihanyu.html">41. 爱汉语语料库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cow.html">42. Chinese Open Wordnet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csdn.html">43. CSDN 下载</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dianchacha.html">44. 店查查数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="../funNLP.html">45. funNLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html">46. 语料库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nlp-datasets.html">47. nlp-datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pku-opendata.html">48. 北京大学开发数据研究平台</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wikipedia.html">49. 维基百科语料库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wordnet.html">50. WordNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/FoolNLTK/index.html">51. FoolNLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/FoolNLTK/train.html">52. train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/JieBa/Jieba.html">53. JieBa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/JieBa/JiebaCpp.html">54. CppJieba</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/JieBa/JiebaNode.html">55. NodeJieba</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/HanLP.html">56. HanLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/Jcseg.html">57. Jcseg logo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/Jiagu.html">58. Jiagu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/Keras.html">59. Keras</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/Keras.html#x-train-and-y-train-are-numpy-arrays-just-like-in-the-scikit-learn-api">60. x_train and y_train are Numpy arrays –just like in the Scikit-Learn API.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/PyTorch.html">61. PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/SnowNLP.html">62. SnowNLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/SpaCy.html">63. spacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/StanfordNLP.html">64. 斯坦福-StanfordNLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/THULAC.html">65. 清华-THULAC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/TensorFlow.html">66. TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/ansj_seg.html">67. Ansj 中文分词</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/awesome.html">68. 👍🏻 表库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/bosonnlp.html">69. 玻森数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/index.html">70. 常用分词工具包</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/ltp.html">71. 哈工大-LTP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/nlpair-ictclas.html">72. 中科院-NLPIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/nltk.html">73. NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/pkuseg.html">74. 北大-pkuseg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/scikit-learn.html">75. scikit-learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/sego.html">76. sego</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Online/index.html">77. 在线分析工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/2018-nlp.html">78. 2018 年，NLP 研究与应用进展到什么水平了？</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/bert-jiqizhixin.html">79. 谷歌终于开源 BERT 代码：3 亿参数量，机器之心全面解读</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/chinese-segmenter.html">80. 细说中文分词</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/index.html">81. 论文 || 文章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/milestone.html">82. 一文看懂 NLP 神经网络发展历史中最重要的 8 个里程碑</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/nlp-gather.html">83. 自然语言处理（NLP）知识结构总结</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/nlp.ict.ac.cn.html">84. 中国科学院计算技术研究所自然语言处理研究组</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sensitive-word/fastscan.html">85. FastScan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sensitive-word/index.html">86. 敏感词</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">87. 术语表</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../contents.html">nlp-docs</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../contents.html">Docs</a> &raquo;</li>
        
      <li>28. 模型</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/Corpus/BERT/model.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1>28. 模型<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h1>
<p>目前有两种多语言模型可供选择。
我们不打算发布更多的单语言模型，但我们将来可能会发布这两种版本的“BERT-Large”版本:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">`BERT-Base,</span> <span class="pre">多语套装(新推荐)</span></code> &lt;<a class="reference external" href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip">https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip</a>&gt;`__:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">`BERT-Base,</span> <span class="pre">多语言Uncased(Orig，不推荐)</span></code> &lt;<a class="reference external" href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip">https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip</a>&gt;`__:
102 languages, 12-layer, 768-hidden, 12-heads, 110M parameters</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">`BERT-Base,</span> <span class="pre">中文</span></code> &lt;<a class="reference external" href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip">https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip</a>&gt;`__:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads,
110M parameters</p></li>
</ul>
<p><strong>“多语言套装(新)”模型还修复了许多语言中的规范化问题，因此建议使用非拉丁字母表的语言(对于大多数使用拉丁字母的语言，它通常更好)。
使用此模型时，请确保将``–do_lower_case = false``传递给``run_pretraining.py``和其他脚本。</strong></p>
<p>请参阅多语言模型支持的<a class="reference external" href="＃list-of-languages">语言列表</a>。
多语言模型确实包含中文(和英文)，但如果您的微调数据仅限中文，则中文模型可能会产生更好的结果。</p>
<div class="section" id="id2">
<h2>28.1. 结果<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<p>为了评估这些系统，我们使用<a class="reference external" href="https://github.com/facebookresearch/XNLI">XNLI
数据集</a>数据集，这是<a class="reference external" href="https://www.nyu.edu/projects/bowman/multinli/">MultiNLI</a>的一个版本其中开发和测试集已经(由人类)翻译成
15 种语言。 请注意，训练集是<em>机器</em>翻译(我们使用的是 XNLI
提供的翻译，而不是 Google NMT)。 为清楚起见，我们仅报告以下 6 种语言:</p>
<!-- mdformat off(没有表格) --><!-- mdformat on --><p>The first two rows are baselines from the XNLI paper and the last three
rows are our results with BERT.</p>
<p><strong>Translate Train</strong> means that the MultiNLI training set was machine
translated from English into the foreign language. So training and
evaluation were both done in the foreign language. Unfortunately,
training was done on machine-translated data, so it is impossible to
quantify how much of the lower accuracy (compared to English) is due to
the quality of the machine translation vs. the quality of the
pre-trained model.</p>
<p><strong>Translate Test</strong> means that the XNLI test set was machine translated
from the foreign language into English. So training and evaluation were
both done on English. However, test evaluation was done on
machine-translated English, so the accuracy depends on the quality of
the machine translation system.</p>
<p><strong>Zero Shot</strong> means that the Multilingual BERT system was fine-tuned on
English MultiNLI, and then evaluated on the foreign language XNLI test.
In this case, machine translation was not involved at all in either the
pre-training or fine-tuning.</p>
<p>Note that the English result is worse than the 84.2 MultiNLI baseline
because this training used Multilingual BERT rather than English-only
BERT. This implies that for high-resource languages, the Multilingual
model is somewhat worse than a single-language model. However, it is not
feasible for us to train and maintain dozens of single-language model.
Therefore, if your goal is to maximize performance with a language other
than English or Chinese, you might find it beneficial to run
pre-training for additional steps starting from our Multilingual model
on data from your language of interest.</p>
<p>Here is a comparison of training Chinese models with the Multilingual
<code class="docutils literal notranslate"><span class="pre">BERT-Base</span></code> and Chinese-only <code class="docutils literal notranslate"><span class="pre">BERT-Base</span></code>:</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 74%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>System</p></th>
<th class="head"><p>Chinese</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>XNLI Baseline</p></td>
<td><p>67.0</p></td>
</tr>
<tr class="row-odd"><td><p>BERT Multilingual Model</p></td>
<td><p>74.2</p></td>
</tr>
<tr class="row-even"><td><p>BERT Chinese-only Model</p></td>
<td><p>77.2</p></td>
</tr>
</tbody>
</table>
<p>Similar to English, the single-language model does 3% better than the
Multilingual model.</p>
</div>
<div class="section" id="id3">
<h2>28.2. 微调示例<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<p>The multilingual model does <strong>not</strong> require any special consideration or
API changes. We did update the implementation of <code class="docutils literal notranslate"><span class="pre">BasicTokenizer</span></code> in
<code class="docutils literal notranslate"><span class="pre">tokenization.py</span></code> to support Chinese character tokenization, so please
update if you forked it. However, we did not change the tokenization
API.</p>
<p>To test the new models, we did modify <code class="docutils literal notranslate"><span class="pre">run_classifier.py</span></code> to add
support for the <a class="reference external" href="https://github.com/facebookresearch/XNLI">XNLI
dataset</a>. This is a
15-language version of MultiNLI where the dev/test sets have been
human-translated, and the training set has been machine-translated.</p>
<p>To run the fine-tuning code, please download the <a class="reference external" href="https://s3.amazonaws.com/xnli/XNLI-1.0.zip">XNLI dev/test
set</a> and the <a class="reference external" href="https://s3.amazonaws.com/xnli/XNLI-MT-1.0.zip">XNLI
machine-translated training
set</a> and then unpack
both .zip files into some directory <code class="docutils literal notranslate"><span class="pre">$XNLI_DIR</span></code>.</p>
<p>To run fine-tuning on XNLI. The language is hard-coded into
<code class="docutils literal notranslate"><span class="pre">run_classifier.py</span></code> (Chinese by default), so please modify
<code class="docutils literal notranslate"><span class="pre">XnliProcessor</span></code> if you want to run on another language.</p>
<p>This is a large dataset, so this will training will take a few hours on
a GPU (or about 30 minutes on a Cloud TPU). To run an experiment quickly
for debugging, just set <code class="docutils literal notranslate"><span class="pre">num_train_epochs</span></code> to a small value like
<code class="docutils literal notranslate"><span class="pre">0.1</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">BERT_BASE_DIR</span><span class="o">=</span>/path/to/bert/chinese_L-12_H-768_A-12 <span class="c1"># or multilingual_L-12_H-768_A-12</span>
<span class="nb">export</span> <span class="nv">XNLI_DIR</span><span class="o">=</span>/path/to/xnli

python run_classifier.py <span class="se">\</span>
  --task_name<span class="o">=</span>XNLI <span class="se">\</span>
  --do_train<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  --do_eval<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  --data_dir<span class="o">=</span><span class="nv">$XNLI_DIR</span> <span class="se">\</span>
  --vocab_file<span class="o">=</span><span class="nv">$BERT_BASE_DIR</span>/vocab.txt <span class="se">\</span>
  --bert_config_file<span class="o">=</span><span class="nv">$BERT_BASE_DIR</span>/bert_config.json <span class="se">\</span>
  --init_checkpoint<span class="o">=</span><span class="nv">$BERT_BASE_DIR</span>/bert_model.ckpt <span class="se">\</span>
  --max_seq_length<span class="o">=</span><span class="m">128</span> <span class="se">\</span>
  --train_batch_size<span class="o">=</span><span class="m">32</span> <span class="se">\</span>
  --learning_rate<span class="o">=</span>5e-5 <span class="se">\</span>
  --num_train_epochs<span class="o">=</span><span class="m">2</span>.0 <span class="se">\</span>
  --output_dir<span class="o">=</span>/tmp/xnli_output/
</pre></div>
</div>
<p>With the Chinese-only model, the results should look something like
this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="o">*****</span> <span class="n">Eval</span> <span class="n">results</span> <span class="o">*****</span>
<span class="n">eval_accuracy</span> <span class="o">=</span> <span class="mf">0.774116</span>
<span class="n">eval_loss</span> <span class="o">=</span> <span class="mf">0.83554</span>
<span class="n">global_step</span> <span class="o">=</span> <span class="mi">24543</span>
<span class="n">loss</span> <span class="o">=</span> <span class="mf">0.74603</span>
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h2>28.3. 细节<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h2>
<div class="section" id="id5">
<h3>28.3.1. 数据源和采样<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h3>
<p>The languages chosen were the <a class="reference external" href="https://meta.wikimedia.org/wiki/List_of_Wikipedias">top 100 languages with the largest
Wikipedias</a>. The
entire Wikipedia dump for each language (excluding user and talk pages)
was taken as the training data for each language</p>
<p>However, the size of the Wikipedia for a given language varies greatly,
and therefore low-resource languages may be “under-represented” in terms
of the neural network model (under the assumption that languages are
“competing” for limited model capacity to some extent).</p>
<p>However, the size of a Wikipedia also correlates with the number of
speakers of a language, and we also don’t want to overfit the model by
performing thousands of epochs over a tiny Wikipedia for a particular
language.</p>
<p>To balance these two factors, we performed exponentially smoothed
weighting of the data during pre-training data creation (and WordPiece
vocab creation). In other words, let’s say that the probability of a
language is <em>P(L)</em>, e.g., <em>P(English) = 0.21</em> means that after
concatenating all of the Wikipedias together, 21% of our data is
English. We exponentiate each probability by some factor <em>S</em> and then
re-normalize, and sample from that distribution. In our case we use
<em>S=0.7</em>. So, high-resource languages like English will be under-sampled,
and low-resource languages like Icelandic will be over-sampled. E.g., in
the original distribution English would be sampled 1000x more than
Icelandic, but after smoothing it’s only sampled 100x more.</p>
</div>
<div class="section" id="id6">
<h3>28.3.2. 符号化<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h3>
<p>For tokenization, we use a 110k shared WordPiece vocabulary. The word
counts are weighted the same way as the data, so low-resource languages
are upweighted by some factor. We intentionally do <em>not</em> use any marker
to denote the input language (so that zero-shot training can work).</p>
<p>Because Chinese (and Japanese Kanji and Korean Hanja) does not have
whitespace characters, we add spaces around every character in the <a class="reference external" href="https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)">CJK
Unicode
range</a>
before applying WordPiece. This means that Chinese is effectively
character-tokenized. Note that the CJK Unicode block only includes
Chinese-origin characters and does <em>not</em> include Hangul Korean or
Katakana/Hiragana Japanese, which are tokenized with
whitespace+WordPiece like all other languages.</p>
<p>For all other languages, we apply the <a class="reference external" href="https://github.com/google-research/bert#tokenization">same recipe as
English</a>: (a)
lower casing+accent removal, (b) punctuation splitting, (c) whitespace
tokenization. We understand that accent markers have substantial meaning
in some languages, but felt that the benefits of reducing the effective
vocabulary make up for this. Generally the strong contextual models of
BERT should make up for any ambiguity introduced by stripping accent
markers.</p>
</div>
<div class="section" id="id7">
<h3>28.3.3. 语言清单<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h3>
<p>多语言模型支持以下语言。 选择这些语言是因为它们是具有最大维基百科的前
100 种语言:</p>
<ul class="simple">
<li><p>Afrikaans</p></li>
<li><p>Albanian</p></li>
<li><p>Arabic</p></li>
<li><p>Aragonese</p></li>
<li><p>Armenian</p></li>
<li><p>Asturian</p></li>
<li><p>Azerbaijani</p></li>
<li><p>Bashkir</p></li>
<li><p>Basque</p></li>
<li><p>Bavarian</p></li>
<li><p>Belarusian</p></li>
<li><p>Bengali</p></li>
<li><p>Bishnupriya Manipuri</p></li>
<li><p>Bosnian</p></li>
<li><p>Breton</p></li>
<li><p>Bulgarian</p></li>
<li><p>Burmese</p></li>
<li><p>Catalan</p></li>
<li><p>Cebuano</p></li>
<li><p>Chechen</p></li>
<li><p>Chinese (Simplified)</p></li>
<li><p>Chinese (Traditional)</p></li>
<li><p>Chuvash</p></li>
<li><p>Croatian</p></li>
<li><p>Czech</p></li>
<li><p>Danish</p></li>
<li><p>Dutch</p></li>
<li><p>English</p></li>
<li><p>Estonian</p></li>
<li><p>Finnish</p></li>
<li><p>French</p></li>
<li><p>Galician</p></li>
<li><p>Georgian</p></li>
<li><p>German</p></li>
<li><p>Greek</p></li>
<li><p>Gujarati</p></li>
<li><p>Haitian</p></li>
<li><p>Hebrew</p></li>
<li><p>Hindi</p></li>
<li><p>Hungarian</p></li>
<li><p>Icelandic</p></li>
<li><p>Ido</p></li>
<li><p>Indonesian</p></li>
<li><p>Irish</p></li>
<li><p>Italian</p></li>
<li><p>Japanese</p></li>
<li><p>Javanese</p></li>
<li><p>Kannada</p></li>
<li><p>Kazakh</p></li>
<li><p>Kirghiz</p></li>
<li><p>Korean</p></li>
<li><p>Latin</p></li>
<li><p>Latvian</p></li>
<li><p>Lithuanian</p></li>
<li><p>Lombard</p></li>
<li><p>Low Saxon</p></li>
<li><p>Luxembourgish</p></li>
<li><p>Macedonian</p></li>
<li><p>Malagasy</p></li>
<li><p>Malay</p></li>
<li><p>Malayalam</p></li>
<li><p>Marathi</p></li>
<li><p>Minangkabau</p></li>
<li><p>Nepali</p></li>
<li><p>Newar</p></li>
<li><p>Norwegian (Bokmal)</p></li>
<li><p>Norwegian (Nynorsk)</p></li>
<li><p>Occitan</p></li>
<li><p>Persian (Farsi)</p></li>
<li><p>Piedmontese</p></li>
<li><p>Polish</p></li>
<li><p>Portuguese</p></li>
<li><p>Punjabi</p></li>
<li><p>Romanian</p></li>
<li><p>Russian</p></li>
<li><p>Scots</p></li>
<li><p>Serbian</p></li>
<li><p>Serbo-Croatian</p></li>
<li><p>Sicilian</p></li>
<li><p>Slovak</p></li>
<li><p>Slovenian</p></li>
<li><p>South Azerbaijani</p></li>
<li><p>Spanish</p></li>
<li><p>Sundanese</p></li>
<li><p>Swahili</p></li>
<li><p>Swedish</p></li>
<li><p>Tagalog</p></li>
<li><p>Tajik</p></li>
<li><p>Tamil</p></li>
<li><p>Tatar</p></li>
<li><p>Telugu</p></li>
<li><p>Turkish</p></li>
<li><p>Ukrainian</p></li>
<li><p>Urdu</p></li>
<li><p>Uzbek</p></li>
<li><p>Vietnamese</p></li>
<li><p>Volapük</p></li>
<li><p>Waray-Waray</p></li>
<li><p>Welsh</p></li>
<li><p>West Frisian</p></li>
<li><p>Western Punjabi</p></li>
<li><p>Yoruba</p></li>
</ul>
<p><strong>多语言套装(新)</strong>版本还包含<strong>泰国</strong>和<strong>蒙古语</strong>，这些都未包含在原始版本中。</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../Knowledge_Graph/Agricultural_Knowledge_Graph.html" class="btn btn-neutral float-right" title="29. 农业知识图谱(AgriKG)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="27. BERT 大规模预训练语言模型" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Nosy

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>