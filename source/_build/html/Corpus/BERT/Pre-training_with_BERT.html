

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>23. ä½¿ç”¨ BERT è¿›è¡Œé¢„è®­ç»ƒ &mdash; nlp-docs v2019.03.19 æ–‡æ¡£</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
    <link rel="index" title="ç´¢å¼•" href="../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../search.html" />
    <link rel="next" title="24. åœ¨ Colab ä¸­ä½¿ç”¨ BERT" href="Using_BERT_in_Colab.html" />
    <link rel="prev" title="22. é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹" href="Pre-trained_models.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../contents.html" class="icon icon-home"> nlp-docs
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../info.html">1. è‡ªç„¶è¯­è¨€å¤„ç†</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Author/index.html">2. è‡ªç„¶è¯­è¨€å¤„ç†ä½œè€…</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/ConditionalRandomField.html">3. æ¡ä»¶éšæœºåœº CRFï¼ˆConditional Random Fieldï¼‰</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Hidden_Markov_Model.html">4. éšé©¬å°”å¯å¤«æ¨¡å‹ HMMï¼ˆHidden Markov Modelï¼‰</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/MMSEG.html">5. MMSEG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/MMSeg_description.html">6. MMSeg åˆ†è¯ç®—æ³•ç®€è¿°</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Maximum-entropy_Markov_model.html">7. æœ€å¤§ç†µé©¬å°”å¯å¤«æ¨¡å‹ MEMMï¼ˆMaximum-entropy Markov modelï¼‰</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Maximum_Entropy.html">8. æœ€å¤§ç†µæ¨¡å‹ MEï¼ˆMaximum Entropyï¼‰</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Statistical_Model.html">9. ç»Ÿè®¡æ¨¡å‹</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/Viterbi.html">10. Viterbi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Algorithm/index.html">11. ç®—æ³•æ±‡æ€»</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/Awesome-Chinese-NLP.html">12. awesome-chinese-nlp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/index.html">13. awesome-nlp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/multi-language.html">14. å…¶ä»–è¯­è¨€</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/techniques.html">15. æŠ€æœ¯</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../awesome/tutorials.html">16. æ•™ç¨‹</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Cloud/aliyun.html">17. é˜¿é‡Œäº‘è‡ªç„¶è¯­è¨€å¤„ç†</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Cloud/pai.html">18. é˜¿é‡Œæœºå™¨å­¦ä¹ å¹³å° PAI 3.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Cloud/ç™¾åº¦AI.html">19. ç™¾åº¦ AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">20. å¸¸é—®é—®é¢˜</a></li>
<li class="toctree-l1"><a class="reference internal" href="Fine-tuning_with_BERT.html">21. ä½¿ç”¨ BERT è¿›è¡Œå¾®è°ƒ</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pre-trained_models.html">22. é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">23. ä½¿ç”¨ BERT è¿›è¡Œé¢„è®­ç»ƒ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">23.1. é¢„è®­ç»ƒæç¤ºå’Œè­¦å‘Š</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id2">23.2. é¢„è®­ç»ƒæ•°æ®</a></li>
<li class="toctree-l2"><a class="reference internal" href="#wordpiece">23.3. å­¦ä¹ ä¸€ä¸ªæ–°çš„ WordPiece è¯æ±‡è¡¨</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Using_BERT_in_Colab.html">24. åœ¨ Colab ä¸­ä½¿ç”¨ BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="Using_BERT_to_extract_fixed_feature_vectors.html">25. ä½¿ç”¨ BERT æå–å›ºå®šçš„ç‰¹å¾å‘é‡ (åƒ ELMo)</a></li>
<li class="toctree-l1"><a class="reference internal" href="What_is_BERT.html">26. ä»€ä¹ˆæ˜¯ BERTï¼Ÿ</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">27. BERT å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">28. æ¨¡å‹</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/Agricultural_Knowledge_Graph.html">29. å†œä¸šçŸ¥è¯†å›¾è°±(AgriKG)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/ChineseNLPCorpus.html">30. Chinese NLP Corpus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/cnSchema.html">31. cnSchema</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/index.html">32. çŸ¥è¯†å›¾è°±</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Knowledge_Graph/openkg.html">33. openkg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regulation/LabelStyle.html">34. ç°ä»£æ±‰è¯­è¯­æ–™åº“åŠ å·¥è§„èŒƒ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regulation/index.html">35. æ ‡æ³¨è§„èŒƒ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regulation/nation.html">36. ä¿¡æ¯å¤„ç†ç”¨ç°ä»£æ±‰è¯­è¯ç±»æ ‡è®°è§„èŒƒ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regulation/nlpir.html">37. è®¡ç®—æ‰€æ±‰è¯­è¯æ€§æ ‡è®°é›†</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GlobalWordNetAssociation.html">38. å…¨çƒ WordNet åä¼š</a></li>
<li class="toctree-l1"><a class="reference internal" href="../HowNet.html">39. çŸ¥ç½‘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Schema.html">40. Schema</a></li>
<li class="toctree-l1"><a class="reference internal" href="../aihanyu.html">41. çˆ±æ±‰è¯­è¯­æ–™åº“</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cow.html">42. Chinese Open Wordnet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csdn.html">43. CSDN ä¸‹è½½</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dianchacha.html">44. åº—æŸ¥æŸ¥æ•°æ®</a></li>
<li class="toctree-l1"><a class="reference internal" href="../funNLP.html">45. funNLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html">46. è¯­æ–™åº“</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nlp-datasets.html">47. nlp-datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pku-opendata.html">48. åŒ—äº¬å¤§å­¦å¼€å‘æ•°æ®ç ”ç©¶å¹³å°</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wikipedia.html">49. ç»´åŸºç™¾ç§‘è¯­æ–™åº“</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wordnet.html">50. WordNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/FoolNLTK/index.html">51. FoolNLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/FoolNLTK/train.html">52. train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/JieBa/Jieba.html">53. JieBa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/JieBa/JiebaCpp.html">54. CppJieba</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/JieBa/JiebaNode.html">55. NodeJieba</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/HanLP.html">56. HanLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/Jcseg.html">57. Jcseg logo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/Jiagu.html">58. Jiagu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/Keras.html">59. Keras</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/Keras.html#x-train-and-y-train-are-numpy-arrays-just-like-in-the-scikit-learn-api">60. x_train and y_train are Numpy arrays â€“just like in the Scikit-Learn API.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/PyTorch.html">61. PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/SnowNLP.html">62. SnowNLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/SpaCy.html">63. spacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/StanfordNLP.html">64. æ–¯å¦ç¦-StanfordNLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/THULAC.html">65. æ¸…å-THULAC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/TensorFlow.html">66. TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/ansj_seg.html">67. Ansj ä¸­æ–‡åˆ†è¯</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/awesome.html">68. ğŸ‘ğŸ» è¡¨åº“</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/bosonnlp.html">69. ç»æ£®æ•°æ®</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/index.html">70. å¸¸ç”¨åˆ†è¯å·¥å…·åŒ…</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/ltp.html">71. å“ˆå·¥å¤§-LTP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/nlpair-ictclas.html">72. ä¸­ç§‘é™¢-NLPIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/nltk.html">73. NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/pkuseg.html">74. åŒ—å¤§-pkuseg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/scikit-learn.html">75. scikit-learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Librariy/sego.html">76. sego</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Online/index.html">77. åœ¨çº¿åˆ†æå·¥å…·</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/2018-nlp.html">78. 2018 å¹´ï¼ŒNLP ç ”ç©¶ä¸åº”ç”¨è¿›å±•åˆ°ä»€ä¹ˆæ°´å¹³äº†ï¼Ÿ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/bert-jiqizhixin.html">79. è°·æ­Œç»ˆäºå¼€æº BERT ä»£ç ï¼š3 äº¿å‚æ•°é‡ï¼Œæœºå™¨ä¹‹å¿ƒå…¨é¢è§£è¯»</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/chinese-segmenter.html">80. ç»†è¯´ä¸­æ–‡åˆ†è¯</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/index.html">81. è®ºæ–‡ || æ–‡ç« </a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/milestone.html">82. ä¸€æ–‡çœ‹æ‡‚ NLP ç¥ç»ç½‘ç»œå‘å±•å†å²ä¸­æœ€é‡è¦çš„ 8 ä¸ªé‡Œç¨‹ç¢‘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/nlp-gather.html">83. è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çŸ¥è¯†ç»“æ„æ€»ç»“</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Paper/nlp.ict.ac.cn.html">84. ä¸­å›½ç§‘å­¦é™¢è®¡ç®—æŠ€æœ¯ç ”ç©¶æ‰€è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶ç»„</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sensitive-word/fastscan.html">85. FastScan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sensitive-word/index.html">86. æ•æ„Ÿè¯</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">87. æœ¯è¯­è¡¨</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../contents.html">nlp-docs</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../contents.html">Docs</a> &raquo;</li>
        
      <li>23. ä½¿ç”¨ BERT è¿›è¡Œé¢„è®­ç»ƒ</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/Corpus/BERT/Pre-training_with_BERT.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="bert">
<h1>23. ä½¿ç”¨ BERT è¿›è¡Œé¢„è®­ç»ƒ<a class="headerlink" href="#bert" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">Â¶</a></h1>
<p>æˆ‘ä»¬æ­£åœ¨å‘å¸ƒä»£ç ï¼Œåœ¨ä»»æ„æ–‡æœ¬è¯­æ–™åº“ä¸Šåšâ€œè’™é¢ LMâ€å’Œâ€œä¸‹ä¸€å¥è¯é¢„æµ‹â€ã€‚
è¯·æ³¨æ„ï¼Œè¿™ä¸æ˜¯ç”¨äºè®ºæ–‡çš„ç¡®åˆ‡ä»£ç ï¼ˆåŸå§‹ä»£ç æ˜¯ç”¨ C
++ç¼–å†™çš„ï¼Œå¹¶ä¸”æœ‰ä¸€äº›é¢å¤–çš„å¤æ‚æ€§ï¼‰ï¼Œä½†æ˜¯æ­¤ä»£ç ç¡®å®ç”Ÿæˆäº†æœ¬æ–‡æ‰€è¿°çš„é¢„è®­ç»ƒæ•°æ®ã€‚</p>
<p>Hereâ€™s how to run the data generation. The input is a plain text file,
with one sentence per line. (It is important that these be actual
sentences for the â€œnext sentence predictionâ€ task). Documents are
delimited by empty lines. The output is a set of <code class="docutils literal notranslate"><span class="pre">tf.train.Example</span></code>s
serialized into <code class="docutils literal notranslate"><span class="pre">TFRecord</span></code> file format.</p>
<p>You can perform sentence segmentation with an off-the-shelf NLP toolkit
such as <a class="reference external" href="https://spacy.io/">spaCy</a>. The
<code class="docutils literal notranslate"><span class="pre">create_pretraining_data.py</span></code> script will concatenate segments until
they reach the maximum sequence length to minimize computational waste
from padding (see the script for more details). However, you may want to
intentionally add a slight amount of noise to your input data (e.g.,
randomly truncate 2% of input segments) to make it more robust to
non-sentential input during fine-tuning.</p>
<p>This script stores all of the examples for the entire input file in
memory, so for large data files you should shard the input file and call
the script multiple times. (You can pass in a file glob to
<code class="docutils literal notranslate"><span class="pre">run_pretraining.py</span></code>, e.g., <code class="docutils literal notranslate"><span class="pre">tf_examples.tf_record*</span></code>.)</p>
<p>The <code class="docutils literal notranslate"><span class="pre">max_predictions_per_seq</span></code> is the maximum number of masked LM
predictions per sequence. You should set this to around
<code class="docutils literal notranslate"><span class="pre">max_seq_length</span></code> * <code class="docutils literal notranslate"><span class="pre">masked_lm_prob</span></code> (the script doesnâ€™t do that
automatically because the exact value needs to be passed to both
scripts).</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python create_pretraining_data.py <span class="se">\</span>
  --input_file<span class="o">=</span>./sample_text.txt <span class="se">\</span>
  --output_file<span class="o">=</span>/tmp/tf_examples.tfrecord <span class="se">\</span>
  --vocab_file<span class="o">=</span><span class="nv">$BERT_BASE_DIR</span>/vocab.txt <span class="se">\</span>
  --do_lower_case<span class="o">=</span>True <span class="se">\</span>
  --max_seq_length<span class="o">=</span><span class="m">128</span> <span class="se">\</span>
  --max_predictions_per_seq<span class="o">=</span><span class="m">20</span> <span class="se">\</span>
  --masked_lm_prob<span class="o">=</span><span class="m">0</span>.15 <span class="se">\</span>
  --random_seed<span class="o">=</span><span class="m">12345</span> <span class="se">\</span>
  --dupe_factor<span class="o">=</span><span class="m">5</span>
</pre></div>
</div>
<p>Hereâ€™s how to run the pre-training. Do not include <code class="docutils literal notranslate"><span class="pre">init_checkpoint</span></code>
if you are pre-training from scratch. The model configuration (including
vocab size) is specified in <code class="docutils literal notranslate"><span class="pre">bert_config_file</span></code>. This demo code only
pre-trains for a small number of steps (20), but in practice you will
probably want to set <code class="docutils literal notranslate"><span class="pre">num_train_steps</span></code> to 10000 steps or more. The
<code class="docutils literal notranslate"><span class="pre">max_seq_length</span></code> and <code class="docutils literal notranslate"><span class="pre">max_predictions_per_seq</span></code> parameters passed to
<code class="docutils literal notranslate"><span class="pre">run_pretraining.py</span></code> must be the same as
<code class="docutils literal notranslate"><span class="pre">create_pretraining_data.py</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python run_pretraining.py <span class="se">\</span>
  --input_file<span class="o">=</span>/tmp/tf_examples.tfrecord <span class="se">\</span>
  --output_dir<span class="o">=</span>/tmp/pretraining_output <span class="se">\</span>
  --do_train<span class="o">=</span>True <span class="se">\</span>
  --do_eval<span class="o">=</span>True <span class="se">\</span>
  --bert_config_file<span class="o">=</span><span class="nv">$BERT_BASE_DIR</span>/bert_config.json <span class="se">\</span>
  --init_checkpoint<span class="o">=</span><span class="nv">$BERT_BASE_DIR</span>/bert_model.ckpt <span class="se">\</span>
  --train_batch_size<span class="o">=</span><span class="m">32</span> <span class="se">\</span>
  --max_seq_length<span class="o">=</span><span class="m">128</span> <span class="se">\</span>
  --max_predictions_per_seq<span class="o">=</span><span class="m">20</span> <span class="se">\</span>
  --num_train_steps<span class="o">=</span><span class="m">20</span> <span class="se">\</span>
  --num_warmup_steps<span class="o">=</span><span class="m">10</span> <span class="se">\</span>
  --learning_rate<span class="o">=</span>2e-5
</pre></div>
</div>
<p>This will produce an output like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">*****</span> <span class="n">Eval</span> <span class="n">results</span> <span class="o">*****</span>
  <span class="n">global_step</span> <span class="o">=</span> <span class="mi">20</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0979674</span>
  <span class="n">masked_lm_accuracy</span> <span class="o">=</span> <span class="mf">0.985479</span>
  <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="mf">0.0979328</span>
  <span class="n">next_sentence_accuracy</span> <span class="o">=</span> <span class="mf">1.0</span>
  <span class="n">next_sentence_loss</span> <span class="o">=</span> <span class="mf">3.45724e-05</span>
</pre></div>
</div>
<p>Note that since our <code class="docutils literal notranslate"><span class="pre">sample_text.txt</span></code> file is very small, this example
training will overfit that data in only a few steps and produce
unrealistically high accuracy numbers.</p>
<div class="section" id="id1">
<h2>23.1. é¢„è®­ç»ƒæç¤ºå’Œè­¦å‘Š<a class="headerlink" href="#id1" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">Â¶</a></h2>
<ul class="simple">
<li><p><strong>If using your own vocabulary, make sure to change ``vocab_size`` in
``bert_config.json``. If you use a larger vocabulary without changing
this, you will likely get NaNs when training on GPU or TPU due to
unchecked out-of-bounds access.</strong></p></li>
<li><p>If your task has a large domain-specific corpus available (e.g.,
â€œmovie reviewsâ€ or â€œscientific papersâ€), it will likely be beneficial
to run additional steps of pre-training on your corpus, starting from
the BERT checkpoint.</p></li>
<li><p>The learning rate we used in the paper was 1e-4. However, if you are
doing additional steps of pre-training starting from an existing BERT
checkpoint, you should use a smaller learning rate (e.g., 2e-5).</p></li>
<li><p>Current BERT models are English-only, but we do plan to release a
multilingual model which has been pre-trained on a lot of languages
in the near future (hopefully by the end of November 2018).</p></li>
<li><p>Longer sequences are disproportionately expensive because attention
is quadratic to the sequence length. In other words, a batch of 64
sequences of length 512 is much more expensive than a batch of 256
sequences of length 128. The fully-connected/convolutional cost is
the same, but the attention cost is far greater for the 512-length
sequences. Therefore, one good recipe is to pre-train for, say,
90,000 steps with a sequence length of 128 and then for 10,000
additional steps with a sequence length of 512. The very long
sequences are mostly needed to learn positional embeddings, which can
be learned fairly quickly. Note that this does require generating the
data twice with different values of <code class="docutils literal notranslate"><span class="pre">max_seq_length</span></code>.</p></li>
<li><p>If you are pre-training from scratch, be prepared that pre-training
is computationally expensive, especially on GPUs. If you are
pre-training from scratch, our recommended recipe is to pre-train a
<code class="docutils literal notranslate"><span class="pre">BERT-Base</span></code> on a single <a class="reference external" href="https://cloud.google.com/tpu/docs/pricing">preemptible Cloud TPU
v2</a>, which takes about
2 weeks at a cost of about $500 USD (based on the pricing in October
2018). You will have to scale down the batch size when only training
on a single Cloud TPU, compared to what was used in the paper. It is
recommended to use the largest batch size that fits into TPU memory.</p></li>
</ul>
</div>
<div class="section" id="id2">
<h2>23.2. é¢„è®­ç»ƒæ•°æ®<a class="headerlink" href="#id2" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">Â¶</a></h2>
<p>We will <strong>not</strong> be able to release the pre-processed datasets used in
the paper. For Wikipedia, the recommended pre-processing is to download
<a class="reference external" href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2">the latest
dump</a>,
extract the text with
<code class="docutils literal notranslate"><span class="pre">`WikiExtractor.py</span></code> &lt;<a class="reference external" href="https://github.com/attardi/wikiextractor">https://github.com/attardi/wikiextractor</a>&gt;`__, and
then apply any necessary cleanup to convert it into plain text.</p>
<p>Unfortunately the researchers who collected the
<a class="reference external" href="http://yknzhu.wixsite.com/mbweb">BookCorpus</a> no longer have it
available for public download. The <a class="reference external" href="https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html">Project Guttenberg
Dataset</a>
is a somewhat smaller (200M word) collection of older books that are
public domain.</p>
<p><a class="reference external" href="http://commoncrawl.org/">Common Crawl</a> is another very large
collection of text, but you will likely have to do substantial
pre-processing and cleanup to extract a usable corpus for pre-training
BERT.</p>
</div>
<div class="section" id="wordpiece">
<h2>23.3. å­¦ä¹ ä¸€ä¸ªæ–°çš„ WordPiece è¯æ±‡è¡¨<a class="headerlink" href="#wordpiece" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">Â¶</a></h2>
<p>This repository does not include code for <em>learning</em> a new WordPiece
vocabulary. The reason is that the code used in the paper was
implemented in C++ with dependencies on Googleâ€™s internal libraries. For
English, it is almost always better to just start with our vocabulary
and pre-trained models. For learning vocabularies of other languages,
there are a number of open source options available. However, keep in
mind that these are not compatible with our <code class="docutils literal notranslate"><span class="pre">tokenization.py</span></code> library:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/google/sentencepiece">Googleâ€™s SentencePiece
library</a></p></li>
<li><p><a class="reference external" href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py">tensor2tensorâ€™s WordPiece generation
script</a></p></li>
<li><p><a class="reference external" href="https://github.com/rsennrich/subword-nmt">Rico Sennrichâ€™s Byte Pair Encoding
library</a></p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Using_BERT_in_Colab.html" class="btn btn-neutral float-right" title="24. åœ¨ Colab ä¸­ä½¿ç”¨ BERT" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Pre-trained_models.html" class="btn btn-neutral float-left" title="22. é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Nosy

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>