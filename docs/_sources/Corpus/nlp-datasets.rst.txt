nlp-datasets
============

`nlp-datasets <https://github.com/niderhoff/nlp-datasets>`__

大量的 nlp 数据集

具有用于自然语言处理（NLP）的文本数据的自由/公共域数据集的字母顺序列表。
这里的大多数内容只是原始的非结构化文本数据，如果您正在寻找带注释的语料库或
Treebanks，请参阅底部的源代码。

英语以及多语言数据集
--------------------

-  `Apache Software Foundation
   公共邮件存档 <http://aws.amazon.com/de/datasets/apache-software-foundation-public-mail-archives/>`__:
   all publicly available Apache Software Foundation mail archives as of
   July 11, 2011 (200 GB)

-  `博客作者语料库 <http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm>`__:
   consists of the collected posts of 19,320 bloggers gathered from
   blogger.com in August 2004. 681,288 posts and over 140 million words.
   (298 MB)

-  `亚马逊美食评论[Kaggle] <https://www.kaggle.com/snap/amazon-fine-food-reviews>`__:
   consists of 568,454 food reviews Amazon users left up to October
   2012. `Paper <http://i.stanford.edu/~julian/pdfs/www13.pdf>`__. (240
   MB)

-  `亚马逊评论 <https://snap.stanford.edu/data/web-Amazon.html>`__:
   Stanford collection of 35 million amazon reviews. (11 GB)

-  `档案 <http://arxiv.org/help/bulk_data_s3>`__: All the Papers on
   archive as fulltext (270 GB) + sourcefiles (190 GB).

-  `尽快自动论文评分[Kaggle] <https://www.kaggle.com/c/asap-aes/data>`__:
   For this competition, there are eight essay sets. Each of the sets of
   essays was generated from a single prompt. Selected essays range from
   an average length of 150 to 550 words per response. Some of the
   essays are dependent upon source information and others are not. All
   responses were written by students ranging in grade levels from Grade
   7 to Grade 10. All essays were hand graded and were double-scored.
   (100 MB)

-  `ASAP 简答题评分[Kaggle] <https://www.kaggle.com/c/asap-sas/data>`__:
   Each of the data sets was generated from a single prompt. Selected
   responses have an average length of 50 words per response. Some of
   the essays are dependent upon source information and others are not.
   All responses were written by students primarily in Grade 10. All
   responses were hand graded and were double-scored. (35 MB)

-  `政治社交媒体的分类 <https://www.crowdflower.com/data-for-everyone/>`__:
   Social media messages from politicians classified by content. (4 MB)

-  `CLiPS
   测量学调查（CSI）语料库 <http://www.clips.uantwerpen.be/datasets/csi-corpus>`__:
   a yearly expanded corpus of student texts in two genres: essays and
   reviews. The purpose of this corpus lies primarily in stylometric
   research, but other applications are possible. (on request)

-  `ClueWeb09 FACC <http://lemurproject.org/clueweb09/FACC1/>`__:
   `ClueWeb09 <http://lemurproject.org/clueweb09/>`__ with Freebase
   annotations (72 GB)

-  `ClueWeb11 FACC <http://lemurproject.org/clueweb12/FACC1/>`__:
   `ClueWeb11 <http://lemurproject.org/clueweb12/>`__ with Freebase
   annotations (92 GB)

-  `常见的爬行语料库 <http://aws.amazon.com/de/datasets/common-crawl-corpus/>`__:
   web crawl data composed of over 5 billion web pages (541 TB)

-  `康奈尔电影对话语料库 <http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html>`__:
   contains a large metadata-rich collection of fictional conversations
   extracted from raw movie scripts: 220,579 conversational exchanges
   between 10,292 pairs of movie characters, 617 movies (9.5 MB)

-  `公司信息 <http://aws.amazon.com/de/datasets/common-crawl-corpus/>`__:
   A data categorization job concerning what corporations actually talk
   about on social media. Contributors were asked to classify statements
   as information (objective statements about the company or it’s
   activities), dialog (replies to users, etc.), or action (messages
   that ask for votes or ask users to click on links, etc.). (600 KB)

-  `Crosswikis <http://nlp.stanford.edu/data/crosswikis-data.tar.bz2/>`__:
   English-phrase-to-associated-Wikipedia-article database. Paper. (11
   GB)

-  `DBpedia <http://aws.amazon.com/de/datasets/dbpedia-3-5-1/?tag=datasets%23keywords%23encyclopedic>`__:
   a community effort to extract structured information from Wikipedia
   and to make this information available on the Web (17 GB)

-  `死囚 <http://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html>`__:
   last words of every inmate executed since 1984 online (HTML table)

-  `Del.icio.us <http://arvindn.livejournal.com/116137.html>`__: 1.25
   million bookmarks on delicious.com (170 MB)

-  `社交媒体上的灾难 <https://www.crowdflower.com/data-for-everyone/>`__:
   10,000 tweets with annotations whether the tweet referred to a
   disaster event (2 MB).

-  `经济新闻文章语气和相关性 <https://www.crowdflower.com/data-for-everyone/>`__:
   News articles judged if relevant to the US economy and, if so, what
   the tone of the article was. Dates range from 1951 to 2014. (12 MB)

-  `安然电子邮件数据 <http://aws.amazon.com/de/datasets/enron-email-data/>`__:
   consists of 1,227,255 emails with 493,384 attachments covering 151
   custodians (210 GB)

-  `事件登记处 <http://eventregistry.org/>`__: Free tool that gives real
   time access to news articles by 100.000 news publishers worldwide.
   `Has API <https://github.com/gregorleban/EventRegistry/>`__. (query
   tool)

-  `Examiner.com - 垃圾邮件 Clickbait
   新闻标题[Kaggle] <https://www.kaggle.com/therohk/examine-the-examiner>`__:
   3 Million crowdsourced News headlines published by now defunct
   clickbait website The Examiner from 2010 to 2015. (200 MB)

-  `联邦采购数据中心的联邦合同（USASpending.gov） <http://aws.amazon.com/de/datasets/federal-contracts-from-the-federal-procurement-data-center-usaspending-gov/>`__:
   data dump of all federal contracts from the Federal Procurement Data
   Center found at USASpending.gov (180 GB)

-  `Flickr
   个人分类法 <http://www.isi.edu/~lerman/downloads/flickr/flickr_taxonomies.html>`__:
   Tree dataset of personal tags (40 MB)

-  `Freebase
   数据转储 <http://aws.amazon.com/de/datasets/freebase-data-dump/>`__:
   data dump of all the current facts and assertions in Freebase (26 GB)

-  `Freebase
   简单主题转储 <http://aws.amazon.com/de/datasets/freebase-simple-topic-dump/>`__:
   data dump of the basic identifying facts about every topic in
   Freebase (5 GB)

-  `Freebase Quad
   Dump <http://aws.amazon.com/de/datasets/freebase-quad-dump/>`__: data
   dump of all the current facts and assertions in Freebase (35 GB)

-  `GigaOM Wordpress
   挑战赛[Kaggle] <https://www.kaggle.com/c/predict-wordpress-likes/data>`__:
   blog posts, meta data, user likes (1.5 GB)

-  `Google Books
   Ngrams <http://storage.googleapis.com/books/ngrams/books/datasetsv2.html>`__:
   available also in hadoop format on amazon s3 (2.2 TB)

-  `Google Web 5gram <https://catalog.ldc.upenn.edu/LDC2006T13>`__:
   contains English word n-grams and their observed frequency counts (24
   GB)

-  `Gutenberg Ebook
   List <http://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs>`__:
   annotated list of ebooks (2 MB)

-  `Hansards
   发表了加拿大议会的大块文章 <http://www.isi.edu/natural-language/download/hansard/>`__:
   1.3 million pairs of aligned text chunks (sentences or smaller
   fragments) from the official records (Hansards) of the 36th Canadian
   Parliament. (82 MB)

-  `哈佛图书馆 <http://library.harvard.edu/open-metadata#Harvard-Library-Bibliographic-Dataset>`__:
   over 12 million bibliographic records for materials held by the
   Harvard Library, including books, journals, electronic resources,
   manuscripts, archival materials, scores, audio, video and other
   materials. (4 GB)

-  `仇恨言论识别 <https://github.com/t-davidson/hate-speech-and-offensive-language>`__:
   Contributors viewed short text and identified if it a) contained hate
   speech, b) was offensive but without hate speech, or c) was not
   offensive at all. Contains nearly 15K rows with three contributor
   judgments per text string. (3 MB)

-  `希拉里克林顿电子邮件[Kaggle] <https://www.kaggle.com/kaggle/hillary-clinton-emails>`__:
   nearly 7,000 pages of Clinton’s heavily redacted emails (12 MB)

-  `历史报纸每年 N-gram
   和实体数据集 <https://data.bris.ac.uk/data/dataset/dobuvuu00mh51q773bo8ybkdz>`__:
   Yearly time series for the usage of the 1,000,000 most frequent 1-,
   2-, and 3-grams from a subset of the British Newspaper Archive
   corpus, along with yearly time series for the 100,000 most frequent
   named entities linked to Wikipedia and a list of all articles and
   newspapers contained in the dataset (3.1 GB)

-  `历史报纸每日词时间序列数据集 <https://datadryad.org/resource/doi:10.5061/dryad.nh775>`__:
   Time series of daily word usage for the 25,000 most frequent words in
   87 years of UK and US historical newspapers between 1836 and 1922.
   (2.7GB)

-  `Home Depot
   产品搜索相关性[Kaggle] <https://www.kaggle.com/c/home-depot-product-search-relevance/data>`__:
   contains a number of products and real customer search terms from
   Home Depot’s website. The challenge is to predict a relevance score
   for the provided combinations of search terms and products. To create
   the ground truth labels, Home Depot has crowdsourced the
   search/product pairs to multiple human raters. (65 MB)

-  `识别文本中的关键短语 <https://www.crowdflower.com/data-for-everyone/>`__:
   Question/Answer pairs + context; context was judged if relevant to
   question/answer. (8 MB)

-  `危险 <http://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/>`__:
   archive of 216,930 past Jeopardy questions (53 MB)

-  `200k 英文明文笑话 <https://github.com/taivop/joke-dataset>`__:
   archive of 208,000 plaintext jokes from various sources.

-  `欧洲语言的机器翻译 <http://statmt.org/wmt11/translation-task.html#download>`__:
   (612 MB)

-  `材料安全数据表 <http://aws.amazon.com/de/datasets/material-safety-data-sheets/>`__:
   230,000 Material Safety Data Sheets. (3 GB)

-  `Million News Headlines - ABC Australia
   [Kaggle] <https://www.kaggle.com/therohk/million-headlines>`__: 1.3
   Million News headlines published by ABC News Australia from 2003 to
   2017. (56 MB)

-  `Millions of News Article
   URLs <https://datadryad.org/resource/doi:10.5061/dryad.p8s0j>`__: 2.3
   million URLs for news articles from the frontpage of over 950
   English-language news outlets in the six month period between October
   2014 and April 2015. (101MB)

-  `MCTest <http://research.microsoft.com/en-us/um/redmond/projects/mctest/index.html>`__:
   a freely available set of 660 stories and associated questions
   intended for research on the machine comprehension of text; for
   question answering (1 MB)

-  `News Headlines of India - Times of India
   [Kaggle] <https://www.kaggle.com/therohk/india-headlines-news-dataset>`__:
   2.7 Million News Headlines with category published by Times of India
   from 2001 to 2017. (185 MB)

-  `News article / Wikipedia page
   pairings <https://www.crowdflower.com/data-for-everyone/>`__:
   Contributors read a short article and were asked which of two
   Wikipedia articles it matched most closely. (6 MB)

-  `NIPS2015 Papers (version 2)
   [Kaggle] <https://www.kaggle.com/benhamner/nips-2015-papers/version/2>`__:
   full text of all NIPS2015 papers (335 MB)

-  `NYTimes Facebook
   Data <http://minimaxir.com/2015/07/facebook-scraper/>`__: all the
   NYTimes facebook posts (5 MB)

-  `One Week of Global News Feeds
   [Kaggle] <https://www.kaggle.com/therohk/global-news-week>`__: News
   Event Dataset of 1.4 Million Articles published globally in 20
   languages over one week of August 2017. (115 MB)

-  `Objective truths of sentences/concept
   pairs <https://www.crowdflower.com/data-for-everyone/>`__:
   Contributors read a sentence with two concepts. For example “a dog is
   a kind of animal” or “captain can have the same meaning as master.”
   They were then asked if the sentence could be true and ranked it on a
   1-5 scale. (700 KB)

-  `Open Library Data
   Dumps <https://openlibrary.org/developers/dumps>`__: dump of all
   revisions of all the records in Open Library. (16 GB)

-  `Personae
   Corpus <http://www.clips.uantwerpen.be/datasets/personae-corpus>`__:
   collected for experiments in Authorship Attribution and Personality
   Prediction. It consists of 145 Dutch-language essays by 145 different
   students. (on request)

-  `Reddit
   Comments <https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/>`__:
   every publicly available reddit comment as of july 2015. 1.7 billion
   comments (250 GB)

-  `Reddit Comments (May ‘15)
   [Kaggle] <https://www.kaggle.com/reddit/reddit-comments-may-2015>`__:
   subset of above dataset (8 GB)

-  `Reddit Submission
   Corpus <https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/>`__:
   all publicly available Reddit submissions from January 2006 - August
   31, 2015). (42 GB)

-  `Reuters Corpus <http://trec.nist.gov/data/reuters/reuters.html>`__:
   a large collection of Reuters News stories for use in research and
   development of natural language processing, information retrieval,
   and machine learning systems. This corpus, known as “Reuters Corpus,
   Volume 1” or RCV1, is significantly larger than the older, well-known
   Reuters-21578 collection heavily used in the text classification
   community. Need to sign agreement and sent per post to obtain. (2.5
   GB)

-  `SMS Spam
   Collection <http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/>`__:
   5,574 English, real and non-enconded SMS messages, tagged according
   being legitimate (ham) or spam. (200 KB)

-  `SouthparkData <https://github.com/BobAdamsEE/SouthParkData>`__: .csv
   files containing script information including: season, episode,
   character, & line. (3.6 MB)

-  `Stanford Question Answering Dataset (SQUAD
   2.0) <https://rajpurkar.github.io/SQuAD-explorer/>`__: a reading
   comprehension dataset, consisting of questions posed by crowdworkers
   on a set of Wikipedia articles, where the answer to every question is
   a segment of text, or span, from the corresponding reading passage,
   or the question might be unanswerable.

-  `Stackoverflow <http://data.stackexchange.com/>`__: 7.3 million
   stackoverflow questions + other stackexchanges (query tool)

-  `Twitter Cheng-Caverlee-Lee
   Scrape <https://archive.org/details/twitter_cikm_2010>`__: Tweets
   from September 2009 - January 2010, geolocated. (400 MB)

-  `Twitter New England Patriots Deflategate
   sentiment <https://www.crowdflower.com/data-for-everyone/>`__: Before
   the 2015 Super Bowl, there was a great deal of chatter around
   deflated footballs and whether the Patriots cheated. This data set
   looks at Twitter sentiment on important days during the scandal to
   gauge public sentiment about the whole ordeal. (2 MB)

-  `Twitter Progressive issues sentiment
   analysis <https://www.crowdflower.com/data-for-everyone/>`__: tweets
   regarding a variety of left-leaning issues like legalization of
   abortion, feminism, Hillary Clinton, etc. classified if the tweets in
   question were for, against, or neutral on the issue (with an option
   for none of the above). (600 KB)

-  `Twitter
   Sentiment140 <http://help.sentiment140.com/for-students/>`__: Tweets
   related to brands/keywords. Website includes papers and research
   ideas. (77 MB)

-  `Twitter sentiment analysis: Self-driving
   cars <https://www.crowdflower.com/data-for-everyone/>`__:
   contributors read tweets and classified them as very positive,
   slightly positive, neutral, slightly negative, or very negative. They
   were also prompted asked to mark if the tweet was not relevant to
   self-driving cars. (1 MB)

-  `Twitter Elections
   Integrity <https://about.twitter.com/en_us/values/elections-integrity.html#data>`__:
   All suspicious tweets and media from 2016 US election. (1.4 GB)

-  `Twitter Tokyo Geolocated
   Tweets <http://followthehashtag.com/datasets/200000-tokyo-geolocated-tweets-free-twitter-dataset/>`__:
   200K tweets from Tokyo. (47 MB)

-  `Twitter UK Geolocated
   Tweets <http://followthehashtag.com/datasets/170000-uk-geolocated-tweets-free-twitter-dataset/>`__:
   170K tweets from UK. (47 MB)

-  `Twitter USA Geolocated
   Tweets <http://followthehashtag.com/datasets/free-twitter-dataset-usa-200000-free-usa-tweets/>`__:
   200k tweets from the US (45MB)

-  `Twitter US Airline Sentiment
   [Kaggle] <https://www.kaggle.com/crowdflower/twitter-airline-sentiment>`__:
   A sentiment analysis job about the problems of each major U.S.
   airline. Twitter data was scraped from February of 2015 and
   contributors were asked to first classify positive, negative, and
   neutral tweets, followed by categorizing negative reasons (such as
   “late flight” or “rude service”). (2.5 MB)

-  `U.S. economic performance based on news
   articles <https://www.crowdflower.com/data-for-everyone/>`__: News
   articles headlines and excerpts ranked as whether relevant to U.S.
   economy. (5 MB)

-  `Urban Dictionary Words and Definitions
   [Kaggle] <https://www.kaggle.com/therohk/urban-dictionary-words-dataset>`__:
   Cleaned CSV corpus of 2.6 Million of all Urban Dictionary words,
   definitions, authors, votes as of May 2016. (238 MB)

-  `Wesbury Lab Usenet
   Corpus <http://aws.amazon.com/de/datasets/the-westburylab-usenet-corpus/>`__:
   anonymized compilation of postings from 47,860 English-language
   newsgroups from 2005-2010 (40 GB)

-  `Wesbury Lab Wikipedia
   Corpus <http://www.psych.ualberta.ca/~westburylab/downloads/westburylab.wikicorp.download.html>`__
   Snapshot of all the articles in the English part of the Wikipedia
   that was taken in April 2010. It was processed, as described in
   detail below, to remove all links and irrelevant material (navigation
   text, etc) The corpus is untagged, raw text. Used by `Stanford
   NLP <https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9060444488071171966&as_sdt=5>`__
   (1.8 GB).

-  `WorldTree Corpus of Explanation Graphs for Elementary Science
   Questions <http://cognitiveai.org/explanationbank/>`__: a corpus of
   manually-constructed explanation graphs, explanatory role ratings,
   and associated semistructured tablestore for most publicly available
   elementary science exam questions in the US (8 MB)

-  `Wikipedia Extraction
   (WEX) <http://aws.amazon.com/de/datasets/wikipedia-extraction-wex/>`__:
   a processed dump of english language wikipedia (66 GB)

-  `Wikipedia XML
   Data <http://aws.amazon.com/de/datasets/wikipedia-xml-data/>`__:
   complete copy of all Wikimedia wikis, in the form of wikitext source
   and metadata embedded in XML. (500 GB)

-  `Yahoo! Answers Comprehensive Questions and
   Answers <http://webscope.sandbox.yahoo.com/catalog.php?datatype=l>`__:
   Yahoo! Answers corpus as of 10/25/2007. Contains 4,483,032 questions
   and their answers. (3.6 GB)

-  `Yahoo! Answers consisting of questions asked in
   French <http://webscope.sandbox.yahoo.com/catalog.php?datatype=l>`__:
   Subset of the Yahoo! Answers corpus from 2006 to 2015 consisting of
   1.7 million questions posed in French, and their corresponding
   answers. (3.8 GB)

-  `Yahoo! Answers Manner
   Questions <http://webscope.sandbox.yahoo.com/catalog.php?datatype=l>`__:
   subset of the Yahoo! Answers corpus from a 10/25/2007 dump, selected
   for their linguistic properties. Contains 142,627 questions and their
   answers. (104 MB)

-  `Yahoo! HTML Forms Extracted from Publicly Available
   Webpages <http://webscope.sandbox.yahoo.com/catalog.php?datatype=l>`__:
   contains a small sample of pages that contain complex HTML forms,
   contains 2.67 million complex forms. (50+ GB)

-  `Yahoo! Metadata Extracted from Publicly Available Web
   Pages <http://webscope.sandbox.yahoo.com/catalog.php?datatype=l>`__:
   100 million triples of RDF data (2 GB)

-  `Yahoo N-Gram
   Representations <http://webscope.sandbox.yahoo.com/catalog.php?datatype=l>`__:
   This dataset contains n-gram representations. The data may serve as a
   testbed for query rewriting task, a common problem in IR research as
   well as to word and sentence similarity task, which is common in NLP
   research. (2.6 GB)

-  `Yahoo! N-Grams, version
   2.0 <http://webscope.sandbox.yahoo.com/catalog.php?datatype=l>`__:
   n-grams (n = 1 to 5), extracted from a corpus of 14.6 million
   documents (126 million unique sentences, 3.4 billion running words)
   crawled from over 12000 news-oriented sites (12 GB)

-  `Yahoo! Search Logs with Relevance
   Judgments <http://webscope.sandbox.yahoo.com/catalog.php?datatype=l>`__:
   Annonymized Yahoo! Search Logs with Relevance Judgments (1.3 GB)

-  `Yahoo! Semantically Annotated Snapshot of the English
   Wikipedia <http://webscope.sandbox.yahoo.com/catalog.php?datatype=l>`__:
   English Wikipedia dated from 2006-11-04 processed with a number of
   publicly-available NLP tools. 1,490,688 entries. (6 GB)

-  `Yelp <https://www.yelp.com/academic_dataset>`__: including
   restaurant rankings and 2.2M reviews (on request)

-  `Youtube <https://www.reddit.com/r/datasets/comments/3gegdz/17_millions_youtube_videos_description/>`__:
   1.7 million youtube videos descriptions (torrent)

资源
----

-  `令人敬畏的公共数据集/
   NLP <https://github.com/caesar0301/awesome-public-datasets#natural-language>`__
   (includes more lists)
-  `AWS 公共数据集 <http://aws.amazon.com/de/datasets/>`__
-  `CrowdFlower: Data for
   Everyone <https://www.crowdflower.com/data-for-everyone/>`__ (lots of
   little surveys they conducted and data obtained by crowdsourcing for
   a specific task)
-  `Kaggle 1 <https://www.kaggle.com/datasets>`__,
   `2 <https://www.kaggle.com/competitions>`__ (make sure though that
   the kaggle competition data can be used outside of the competition!)
-  `Open Library <https://openlibrary.org/developers/dumps>`__
-  `Quora <https://www.quora.com/Datasets-What-are-the-major-text-corpora-used-by-computational-linguists-and-natural-language-processing-researchers-and-what-are-the-characteristics-biases-of-each-corpus>`__
   (mainly annotated corpora)
-  `/r/datasets <https://www.reddit.com/r/datasets>`__ (endless list of
   datasets, most is scraped by amateurs though and not properly
   documented or licensed)
-  `rs.io <http://rs.io/100-interesting-data-sets-for-statistics/>`__
   (another big list)
-  `Stackexchange: Opendata <http://opendata.stackexchange.com/>`__
-  `Stanford NLP
   group <http://www-nlp.stanford.edu/links/statnlp.html>`__ (mainly
   annotated corpora and TreeBanks or actual NLP tools)
-  `Yahoo! Webscope <http://webscope.sandbox.yahoo.com/>`__ (also
   includes papers that use the data that is provided)

阿拉伯数据集
------------

-  `SaudiNewsNet <https://github.com/ParallelMazen/SaudiNewsNet>`__:
   31,030 Arabic newspaper articles alongwith metadata, extracted from
   various online Saudi newspapers. (2 MB)

德语数据集
----------

-  `German Political Speeches
   Corpus <http://purl.org/corpus/german-speeches>`__: collection of
   recent speeches held by top German representatives (25 MB, 11
   MTokens)

-  `NEGRA <http://www.coli.uni-saarland.de/projects/sfb378/negra-corpus/negra-corpus.html>`__:
   A Syntactically Annotated Corpus of German Newspaper Texts. Available
   for free for all Universities and non-profit organizations. Need to
   sign and send form to obtain. (on request)

-  `Ten Thousand German News Articles
   Dataset <https://tblock.github.io/10kGNAD/>`__: 10273 german language
   news articles categorized into nine classes for topic classification.
   (26.1 MB)
